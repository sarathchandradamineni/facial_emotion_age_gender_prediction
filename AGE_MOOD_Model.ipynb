{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtWidgets import *\n",
    "from PyQt5.QtGui import *\n",
    "from PyQt5.QtCore import Qt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pyagender import PyAgender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\anaconda3\\envs\\newface\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from fer import FER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,468 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,471 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,474 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,505 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,507 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,508 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,516 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,518 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,530 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,541 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:03,653 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:04,333 WARNING  [module_wrapper.py:139] From C:\\Users\\UTVTRA\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agender = PyAgender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#face detection model\n",
    "faceProto=\"opencv_face_detector.pbtxt\"\n",
    "faceModel=\"opencv_face_detector_uint8.pb\"\n",
    "faceNet=cv2.dnn.readNet(faceModel,faceProto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender detection\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "genderNet=cv2.dnn.readNet(genderModel,genderProto)\n",
    "genderList=['Male','Female']\n",
    "MODEL_MEAN_VALUES=(78.4263377603,87.7689143744,114.895847746)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harcascade face detection\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_face(frame):\n",
    "    new_frame = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor = 1.3, minNeighbors = 5, minSize=(30,30))\n",
    "    frameWidth=new_frame.shape[1]\n",
    "    \n",
    "    face_list = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(new_frame, (x,y),(x+w, y+h),(40, 40, 40),2)\n",
    "        face_list.append([x,y,x+w,y+h])\n",
    "\n",
    "    \n",
    "    return new_frame, face_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#highlights the face and also returns the frame with placing the rectangles on the faces\n",
    "def highlightFace(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn=frame.copy()\n",
    "    frameHeight=frameOpencvDnn.shape[0]\n",
    "    frameWidth=frameOpencvDnn.shape[1]\n",
    "    blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "    \n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections=net.forward()\n",
    "    faceBoxes=[]\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence=detections[0,0,i,2]\n",
    "\n",
    "        if confidence>conf_threshold:\n",
    "            x1=int(detections[0,0,i,3]*frameWidth)\n",
    "            y1=int(detections[0,0,i,4]*frameHeight)\n",
    "            x2=int(detections[0,0,i,5]*frameWidth)\n",
    "            y2=int(detections[0,0,i,6]*frameHeight)\n",
    "\n",
    "            faceBoxes.append([x1,y1,x2,y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (40,40,40), int(round(frameHeight/150)), 1)\n",
    "    \n",
    "    #faceBoxes = face_recognition.face_locations(frameOpencvDnn)\n",
    "    \n",
    "    #for each_face in faceBoxes:\n",
    "        \n",
    "    \n",
    "\n",
    "    return frameOpencvDnn,faceBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def detectFace():\\n    while cv2.waitKey(1) < 0:\\n        padding = 20\\n        hasFrame,frame=video.read()\\n        resultImg, faceBoxes = highlightFace(faceNet,frame)\\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\\n        \\n        if(len(faceBoxes) != 0):\\n\\n            for faceBox in faceBoxes:\\n                face = frame[max(0,faceBox[1]-padding):min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding):min(faceBox[2]+padding,frame.shape[1]-1)]\\n                blob = cv2.dnn.blobFromImage(face,1.0,(227,227),MODEL_MEAN_VALUES,swapRB=False)\\n                genderNet.setInput(blob)\\n                genderPreds = genderNet.forward()\\n                gender = genderList[genderPreds[0].argmax()]\\n                faces_agender = agender.detect_genders_ages(resultImg)\\n                print(faces_agender[0]['age'])\\n                try:\\n                    #expression = DeepFace.analyze(resultImg, actions = ['emotion'])\\n                    cv2.rectangle(resultImg,(faceBox[2]+5,faceBox[1]),(faceBox[2]+120,faceBox[1]+30),(128,128,128),-1)\\n                    cv2.putText(resultImg, 'Gender: '+gender,(faceBox[2]+7, faceBox[1]+10),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\\n                    #cv2.putText(resultImg, 'Emotion: '+max(expression['emotion'].items(), key=operator.itemgetter(1))[0],(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\\n                    cv2.putText(resultImg, 'Age: '+faces_agender[0]['age'],(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\\n                    #cv2.putText(resultImg,max(expression['emotion'].items(), key=operator.itemgetter(1))[0], (faceBox[0], faceBox[1]+15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\\n                except Exception as e:\\n                    print('gender not found by model')\\n        cv2.imshow('faces', resultImg)\\n        \\n        if cv2.waitKey(1) & 0xFF == ord('q'):\\n            cv2.destroyWindow('faces')\\n            cv2.waitKey(1)\\n            video.release\\n            break \""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def detectFace():\n",
    "    while cv2.waitKey(1) < 0:\n",
    "        padding = 20\n",
    "        hasFrame,frame=video.read()\n",
    "        resultImg, faceBoxes = highlightFace(faceNet,frame)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if(len(faceBoxes) != 0):\n",
    "\n",
    "            for faceBox in faceBoxes:\n",
    "                face = frame[max(0,faceBox[1]-padding):min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding):min(faceBox[2]+padding,frame.shape[1]-1)]\n",
    "                blob = cv2.dnn.blobFromImage(face,1.0,(227,227),MODEL_MEAN_VALUES,swapRB=False)\n",
    "                genderNet.setInput(blob)\n",
    "                genderPreds = genderNet.forward()\n",
    "                gender = genderList[genderPreds[0].argmax()]\n",
    "                faces_agender = agender.detect_genders_ages(resultImg)\n",
    "                print(faces_agender[0]['age'])\n",
    "                try:\n",
    "                    #expression = DeepFace.analyze(resultImg, actions = ['emotion'])\n",
    "                    cv2.rectangle(resultImg,(faceBox[2]+5,faceBox[1]),(faceBox[2]+120,faceBox[1]+30),(128,128,128),-1)\n",
    "                    cv2.putText(resultImg, 'Gender: '+gender,(faceBox[2]+7, faceBox[1]+10),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "                    #cv2.putText(resultImg, 'Emotion: '+max(expression['emotion'].items(), key=operator.itemgetter(1))[0],(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(resultImg, 'Age: '+faces_agender[0]['age'],(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "                    #cv2.putText(resultImg,max(expression['emotion'].items(), key=operator.itemgetter(1))[0], (faceBox[0], faceBox[1]+15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "                except Exception as e:\n",
    "                    print('gender not found by model')\n",
    "        cv2.imshow('faces', resultImg)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyWindow('faces')\n",
    "            cv2.waitKey(1)\n",
    "            video.release\n",
    "            break '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the images from the files\n",
    "happy_img = cv2.imread('happy2.jpg')\n",
    "sad_img = cv2.imread('sad2.jpg')\n",
    "anger_img = cv2.imread('angry2.jpg')\n",
    "neutral_img = cv2.imread('neutral2.jpg')\n",
    "surprise_img = cv2.imread('surprise2.jpg')\n",
    "\n",
    "#Method to resize the emojis with width 50 and height 50\n",
    "happy = cv2.resize(happy_img,(50,50))\n",
    "sad = cv2.resize(sad_img,(50,50))\n",
    "anger = cv2.resize(anger_img,(50,50))\n",
    "neutral = cv2.resize(neutral_img,(50,50))\n",
    "surprise = cv2.resize(surprise_img,(50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UTVTRA\\anaconda3\\envs\\newface\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21-01-2021:15:37:21,683 WARNING  [deprecation.py:506] From C:\\Users\\UTVTRA\\anaconda3\\envs\\newface\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#detectFace()\n",
    "detector = FER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model testing\n",
    "sarath_img = cv2.imread('sarath.jfif')\n",
    "faces_sarath = agender.detect_genders_ages(sarath_img)\n",
    "emo = detector.top_emotion(sarath_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32, 135, 94, 73)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#face location with new face recognition package\n",
    "face_location = face_recognition.face_locations(sarath_img)\n",
    "face_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'left': 41,\n",
       "  'top': 0,\n",
       "  'right': 170,\n",
       "  'bottom': 126,\n",
       "  'width': 129,\n",
       "  'height': 126,\n",
       "  'gender': 0.113611184,\n",
       "  'age': 31.368002505971162}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces_sarath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'left': 41,\n",
       "  'top': 0,\n",
       "  'right': 170,\n",
       "  'bottom': 126,\n",
       "  'width': 129,\n",
       "  'height': 126,\n",
       "  'gender': 0.113611184,\n",
       "  'age': 31.368002505971162}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Py-Agender testing\n",
    "faces_sarath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('happy', 0.53)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#emotion-testing\n",
    "emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_1080():\n",
    "    video.set(3,1920)\n",
    "    video.set(4,1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_720():\n",
    "    video.set(3,1280)\n",
    "    video.set(4,720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TV():\n",
    "    video.set(3,1440)\n",
    "    video.set(4,2560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_special():\n",
    "    video.set(3, 2200)\n",
    "    video.set(4, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_1080()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_TV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_special()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[894, 327, 1254, 801]]\n",
      "[]\n",
      "exception thrown\n",
      "[[903, 331, 1272, 791]]\n",
      "[]\n",
      "exception thrown\n",
      "[[899, 329, 1269, 799]]\n",
      "[]\n",
      "exception thrown\n",
      "[[900, 329, 1266, 807]]\n",
      "[{'left': 0, 'top': 5, 'right': 604, 'bottom': 716, 'width': 604, 'height': 711, 'gender': 0.026248481, 'age': 30.172522560561447}]\n",
      "[[888, 324, 1256, 817]]\n",
      "exception thrown\n",
      "[[890, 325, 1257, 815]]\n",
      "exception thrown\n",
      "[[894, 323, 1261, 808]]\n",
      "exception thrown\n",
      "[[893, 322, 1259, 807]]\n",
      "exception thrown\n",
      "[[881, 324, 1249, 805]]\n",
      "exception thrown\n",
      "[[880, 321, 1248, 809]]\n",
      "exception thrown\n",
      "[[875, 319, 1248, 807]]\n",
      "[]\n",
      "exception thrown\n",
      "[[876, 316, 1239, 787]]\n",
      "[{'left': 0, 'top': 0, 'right': 597, 'bottom': 705, 'width': 597, 'height': 705, 'gender': 0.013909101, 'age': 28.402071922210325}]\n",
      "[[866, 306, 1231, 773]]\n",
      "[]\n",
      "exception thrown\n",
      "[[870, 308, 1231, 787]]\n",
      "[{'left': 0, 'top': 0, 'right': 599, 'bottom': 717, 'width': 599, 'height': 717, 'gender': 0.014541059, 'age': 29.50153840938492}]\n",
      "[[866, 309, 1224, 778]]\n",
      "[{'left': 0, 'top': 0, 'right': 592, 'bottom': 703, 'width': 592, 'height': 703, 'gender': 0.019336157, 'age': 28.419490732955637}]\n",
      "[[868, 302, 1227, 774]]\n",
      "[{'left': 0, 'top': 0, 'right': 594, 'bottom': 707, 'width': 594, 'height': 707, 'gender': 0.009980597, 'age': 30.84314876409917}]\n",
      "[[871, 300, 1223, 780]]\n",
      "[{'left': 0, 'top': 0, 'right': 591, 'bottom': 719, 'width': 591, 'height': 719, 'gender': 0.010586929, 'age': 28.395830735507843}]\n",
      "[[862, 293, 1223, 770]]\n",
      "[{'left': 0, 'top': 0, 'right': 599, 'bottom': 715, 'width': 599, 'height': 715, 'gender': 0.020749627, 'age': 27.576397257062126}]\n",
      "[[851, 277, 1192, 781]]\n",
      "[{'left': 0, 'top': 0, 'right': 592, 'bottom': 755, 'width': 592, 'height': 755, 'gender': 0.007389949, 'age': 26.152953791689697}]\n",
      "[[849, 274, 1191, 780]]\n",
      "[{'left': 0, 'top': 0, 'right': 594, 'bottom': 758, 'width': 594, 'height': 758, 'gender': 0.004426503, 'age': 27.70266096769501}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[854, 307, 1188, 777]]\n",
      "[{'left': 0, 'top': 0, 'right': 568, 'bottom': 704, 'width': 568, 'height': 704, 'gender': 0.006111246, 'age': 25.890679245124375}]\n",
      "[[850, 295, 1194, 759]]\n",
      "[{'left': 0, 'top': 0, 'right': 575, 'bottom': 695, 'width': 575, 'height': 695, 'gender': 0.0063370843, 'age': 28.929224229664214}]\n",
      "[[881, 283, 1234, 758]]\n",
      "[{'left': 0, 'top': 0, 'right': 589, 'bottom': 711, 'width': 589, 'height': 711, 'gender': 0.00538021, 'age': 27.380272733621496}]\n",
      "[[899, 300, 1256, 760]]\n",
      "[{'left': 0, 'top': 0, 'right': 586, 'bottom': 689, 'width': 586, 'height': 689, 'gender': 0.010755113, 'age': 28.005195995322538}]\n",
      "[[888, 305, 1257, 762]]\n",
      "exception thrown\n",
      "[[896, 321, 1260, 793]]\n",
      "exception thrown\n",
      "[]\n",
      "[[909, 326, 1271, 798]]\n",
      "exception thrown\n",
      "[]\n",
      "[]\n",
      "[[907, 329, 1268, 802]]\n",
      "exception thrown\n",
      "[[908, 330, 1267, 804]]\n",
      "[]\n",
      "exception thrown\n"
     ]
    }
   ],
   "source": [
    "while cv2.waitKey(1) < 0:\n",
    "    hasFrame,frame = video.read()\n",
    "    resultImg, faceBoxes = highlightFace(faceNet,frame)\n",
    "    #resultImg, faceBoxes = new_face(frame)\n",
    "    print(faceBoxes)\n",
    "    for faceBox in faceBoxes:\n",
    "        padding = (faceBox[3]-faceBox[1])*0.25\n",
    "        face = frame[math.ceil(max(0,faceBox[1]-padding)):math.ceil(min(faceBox[3]+padding,frame.shape[0]-1)),math.ceil(max(0,faceBox[0]-padding)):math.ceil(min(faceBox[2]+padding,frame.shape[1]-1))]\n",
    "        \n",
    "        try:\n",
    "            blob = cv2.dnn.blobFromImage(face,1.0,(227,227),MODEL_MEAN_VALUES,swapRB=False)\n",
    "            genderNet.setInput(blob)\n",
    "\n",
    "            genderPreds = genderNet.forward()\n",
    "            gender = genderList[genderPreds[0].argmax()]\n",
    "            faces_agender = agender.detect_genders_ages(face)\n",
    "            emotion, score = detector.top_emotion(face)\n",
    "            \n",
    "            print(faces_agender)\n",
    "            cv2.rectangle(resultImg,(faceBox[2]+5,faceBox[1]),(faceBox[2]+70,faceBox[1]+30),(50,255,255),-1)\n",
    "            #cv2.rectangle(frame,(faces_agender[0]['left'],faces_agender[0]['top']),(faces_agender[0]['right'],faces_agender[0]['bottom']),int(round(faces_agender[0]['bottom']-faces_agender[0]['top']/150)),-1)\n",
    "            cv2.putText(resultImg, gender+', '+str(math.floor(faces_agender[0]['age'])),(faceBox[2]+7, faceBox[1]+10),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1, cv2.LINE_AA)\n",
    "            #cv2.putText(resultImg, 'Age: '+str(math.floor(faces_agender[0]['age'])),(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            \n",
    "            background_img = resultImg\n",
    "            \n",
    "            if(emotion == \"happy\"):\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, happy[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg, \"Happy\",(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1, cv2.LINE_AA)\n",
    "            elif(emotion == \"neutral\"): #neutral\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, neutral[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg, \"neutral\",(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1, cv2.LINE_AA)\n",
    "            elif(emotion == \"sad\" or emotion == \"disgust\" or emotion == \"fear\"): #sad\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, sad[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg, \"sad\",(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1, cv2.LINE_AA)\n",
    "            elif(emotion == \"angry\"): #angry\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, anger[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg,\"angry\",(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1, cv2.LINE_AA)\n",
    "            elif(emotion == \"surprise\"): #angry\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, anger[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                v2.putText(resultImg,\"surprise\",(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1, cv2.LINE_AA)\n",
    "            cv2.imshow('detect age',background_img)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print('exception thrown')\n",
    "            cv2.namedWindow('detect age',cv2.WINDOW_NORMAL)\n",
    "            #cv2.setWindowProperty('detect age',cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "            cv2.imshow('detect age',resultImg)\n",
    "            \n",
    "    cv2.namedWindow('detect age',cv2.WINDOW_NORMAL)\n",
    "    #cv2.setWindowProperty('detect age',cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)        \n",
    "    cv2.imshow('detect age',resultImg)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyWindow('detect age')\n",
    "        cv2.waitKey(1)\n",
    "        video.release\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[[654 343 317 317]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[887, 461, 1282, 856]]\n",
      "[{'left': 0, 'top': 0, 'right': 394, 'bottom': 394, 'width': 394, 'height': 394, 'gender': 0.18944247, 'age': 33.936352607634035}]\n",
      "[[814, 432, 1241, 859]]\n",
      "[{'left': 0, 'top': 0, 'right': 426, 'bottom': 426, 'width': 426, 'height': 426, 'gender': 0.048286777, 'age': 30.502220653299446}]\n",
      "[[832, 452, 1243, 863]]\n",
      "[{'left': 0, 'top': 0, 'right': 410, 'bottom': 410, 'width': 410, 'height': 410, 'gender': 0.046186417, 'age': 32.011799362284364}]\n",
      "[[847, 461, 1236, 850]]\n",
      "[{'left': 0, 'top': 0, 'right': 388, 'bottom': 388, 'width': 388, 'height': 388, 'gender': 0.07206063, 'age': 33.41393216892175}]\n",
      "[[826, 455, 1223, 852]]\n",
      "[{'left': 0, 'top': 0, 'right': 396, 'bottom': 396, 'width': 396, 'height': 396, 'gender': 0.06394355, 'age': 28.870249679395783}]\n",
      "[[816, 451, 1224, 859]]\n",
      "[{'left': 0, 'top': 0, 'right': 407, 'bottom': 407, 'width': 407, 'height': 407, 'gender': 0.065296724, 'age': 27.897979507250966}]\n",
      "[[835, 463, 1206, 834]]\n",
      "[{'left': 0, 'top': 0, 'right': 370, 'bottom': 370, 'width': 370, 'height': 370, 'gender': 0.11723792, 'age': 27.412356319073297}]\n",
      "[[828, 428, 1221, 821]]\n",
      "[{'left': 0, 'top': 0, 'right': 392, 'bottom': 392, 'width': 392, 'height': 392, 'gender': 0.08750988, 'age': 30.775923681045242}]\n",
      "[[823, 429, 1218, 824]]\n",
      "[{'left': 0, 'top': 0, 'right': 394, 'bottom': 394, 'width': 394, 'height': 394, 'gender': 0.09564193, 'age': 30.486422057678283}]\n",
      "[[823, 432, 1211, 820]]\n",
      "[{'left': 0, 'top': 0, 'right': 387, 'bottom': 387, 'width': 387, 'height': 387, 'gender': 0.07491351, 'age': 33.04655248152994}]\n",
      "[[823, 436, 1201, 814]]\n",
      "[{'left': 0, 'top': 0, 'right': 377, 'bottom': 377, 'width': 377, 'height': 377, 'gender': 0.13598232, 'age': 34.71516123240326}]\n",
      "[[821, 429, 1216, 824]]\n",
      "[{'left': 0, 'top': 0, 'right': 394, 'bottom': 394, 'width': 394, 'height': 394, 'gender': 0.10631891, 'age': 30.689563485328108}]\n",
      "[[822, 431, 1211, 820]]\n",
      "[{'left': 0, 'top': 0, 'right': 388, 'bottom': 388, 'width': 388, 'height': 388, 'gender': 0.12529379, 'age': 30.73910043056094}]\n",
      "[[823, 435, 1206, 818]]\n",
      "[{'left': 0, 'top': 0, 'right': 382, 'bottom': 382, 'width': 382, 'height': 382, 'gender': 0.12633605, 'age': 32.059637896447384}]\n",
      "[[821, 434, 1204, 817]]\n",
      "[{'left': 0, 'top': 0, 'right': 382, 'bottom': 382, 'width': 382, 'height': 382, 'gender': 0.13766745, 'age': 30.173962743985612}]\n",
      "[[820, 435, 1200, 815]]\n",
      "[{'left': 0, 'top': 0, 'right': 379, 'bottom': 379, 'width': 379, 'height': 379, 'gender': 0.21325363, 'age': 30.710802806774154}]\n",
      "[[821, 427, 1213, 819]]\n",
      "[{'left': 0, 'top': 0, 'right': 391, 'bottom': 391, 'width': 391, 'height': 391, 'gender': 0.15793557, 'age': 31.0881108930771}]\n",
      "[[818, 432, 1204, 818]]\n",
      "[{'left': 0, 'top': 0, 'right': 385, 'bottom': 385, 'width': 385, 'height': 385, 'gender': 0.15236059, 'age': 28.332136365064798}]\n",
      "[[807, 423, 1219, 835]]\n",
      "[{'left': 0, 'top': 0, 'right': 411, 'bottom': 411, 'width': 411, 'height': 411, 'gender': 0.07853541, 'age': 30.02498050086251}]\n",
      "[[822, 430, 1210, 818]]\n",
      "[{'left': 0, 'top': 0, 'right': 387, 'bottom': 387, 'width': 387, 'height': 387, 'gender': 0.16949655, 'age': 31.080127484154218}]\n",
      "[[826, 437, 1204, 815]]\n",
      "[{'left': 0, 'top': 0, 'right': 377, 'bottom': 377, 'width': 377, 'height': 377, 'gender': 0.16690692, 'age': 31.233088298798975}]\n",
      "[[819, 434, 1199, 814]]\n",
      "[{'left': 0, 'top': 0, 'right': 379, 'bottom': 379, 'width': 379, 'height': 379, 'gender': 0.11332128, 'age': 33.14486883570862}]\n",
      "[[820, 434, 1207, 821]]\n",
      "[{'left': 0, 'top': 0, 'right': 386, 'bottom': 386, 'width': 386, 'height': 386, 'gender': 0.16277374, 'age': 32.41911049326882}]\n",
      "[[818, 428, 1208, 818]]\n",
      "[{'left': 0, 'top': 0, 'right': 389, 'bottom': 389, 'width': 389, 'height': 389, 'gender': 0.12293086, 'age': 32.454295579971586}]\n",
      "[[818, 430, 1210, 822]]\n",
      "[{'left': 0, 'top': 0, 'right': 391, 'bottom': 391, 'width': 391, 'height': 391, 'gender': 0.102409706, 'age': 32.08111272420865}]\n",
      "[[814, 431, 1204, 821]]\n",
      "[{'left': 0, 'top': 0, 'right': 389, 'bottom': 389, 'width': 389, 'height': 389, 'gender': 0.10915618, 'age': 30.587392663856008}]\n",
      "[[810, 425, 1207, 822]]\n",
      "[{'left': 0, 'top': 0, 'right': 396, 'bottom': 396, 'width': 396, 'height': 396, 'gender': 0.12725975, 'age': 31.77452391148472}]\n",
      "[[824, 443, 1195, 814]]\n",
      "[{'left': 0, 'top': 0, 'right': 370, 'bottom': 370, 'width': 370, 'height': 370, 'gender': 0.24291682, 'age': 31.650914294903487}]\n",
      "[[824, 441, 1196, 813]]\n",
      "[{'left': 0, 'top': 0, 'right': 371, 'bottom': 371, 'width': 371, 'height': 371, 'gender': 0.1776836, 'age': 31.19597873683597}]\n",
      "[[816, 433, 1206, 823]]\n",
      "[{'left': 0, 'top': 0, 'right': 389, 'bottom': 389, 'width': 389, 'height': 389, 'gender': 0.1116833, 'age': 29.958816129179468}]\n",
      "[[814, 427, 1213, 826]]\n",
      "[{'left': 0, 'top': 0, 'right': 398, 'bottom': 398, 'width': 398, 'height': 398, 'gender': 0.08600847, 'age': 29.64366761106612}]\n",
      "[[820, 440, 1200, 820]]\n",
      "[{'left': 0, 'top': 0, 'right': 379, 'bottom': 379, 'width': 379, 'height': 379, 'gender': 0.15625374, 'age': 31.73817001820862}]\n",
      "[[819, 433, 1207, 821]]\n",
      "[{'left': 0, 'top': 0, 'right': 387, 'bottom': 387, 'width': 387, 'height': 387, 'gender': 0.12544212, 'age': 32.47294967580092}]\n",
      "[[816, 434, 1204, 822]]\n",
      "[{'left': 0, 'top': 0, 'right': 387, 'bottom': 387, 'width': 387, 'height': 387, 'gender': 0.12523015, 'age': 34.5725673775014}]\n",
      "[[816, 438, 1196, 818]]\n",
      "[{'left': 0, 'top': 0, 'right': 379, 'bottom': 379, 'width': 379, 'height': 379, 'gender': 0.13834432, 'age': 32.11243167244902}]\n",
      "[[814, 449, 1186, 821]]\n",
      "[{'left': 0, 'top': 0, 'right': 371, 'bottom': 371, 'width': 371, 'height': 371, 'gender': 0.21256058, 'age': 33.13190734042291}]\n",
      "[[821, 444, 1240, 863]]\n",
      "[{'left': 0, 'top': 0, 'right': 418, 'bottom': 418, 'width': 418, 'height': 418, 'gender': 0.077892266, 'age': 28.42117151342245}]\n"
     ]
    }
   ],
   "source": [
    "while cv2.waitKey(1) < 0:\n",
    "    hasFrame,frame = video.read()\n",
    "    #resultImg, faceBoxes = highlightFace(faceNet,frame)\n",
    "    resultImg, faceBoxes = new_face(frame)\n",
    "    print(faceBoxes)\n",
    "    for faceBox in faceBoxes:\n",
    "        padding = (faceBox[3]-faceBox[1])*0\n",
    "        face = frame[math.ceil(max(0,faceBox[1]-padding)):math.ceil(min(faceBox[3]+padding,frame.shape[0]-1)),math.ceil(max(0,faceBox[0]-padding)):math.ceil(min(faceBox[2]+padding,frame.shape[1]-1))]\n",
    "        \n",
    "        try:\n",
    "            cv2.namedWindow('',flags=cv2.WINDOW_GUI_NORMAL)\n",
    "            blob = cv2.dnn.blobFromImage(face,1.0,(227,227),MODEL_MEAN_VALUES,swapRB=False)\n",
    "            genderNet.setInput(blob)\n",
    "\n",
    "            genderPreds = genderNet.forward()\n",
    "            gender = genderList[genderPreds[0].argmax()]\n",
    "            faces_agender = agender.detect_genders_ages(face)\n",
    "            emotion, score = detector.top_emotion(face)\n",
    "            \n",
    "            print(faces_agender)\n",
    "            cv2.rectangle(resultImg,(faceBox[2]+5,faceBox[1]),(faceBox[2]+140,faceBox[1]+70),(50,255,255),-1)\n",
    "            #cv2.rectangle(frame,(faces_agender[0]['left'],faces_agender[0]['top']),(faces_agender[0]['right'],faces_agender[0]['bottom']),int(round(faces_agender[0]['bottom']-faces_agender[0]['top']/150)),-1)\n",
    "            cv2.putText(resultImg, gender+', '+str(math.floor(faces_agender[0]['age'])),(faceBox[2]+7, faceBox[1]+22),cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2, cv2.LINE_AA)\n",
    "            #cv2.putText(resultImg, 'Age: '+str(math.floor(faces_agender[0]['age'])),(faceBox[2]+7, faceBox[1]+25),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            \n",
    "            background_img = resultImg\n",
    "            \n",
    "            if(emotion == \"happy\"):\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, happy[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg, \"Happy\",(faceBox[2]+7, faceBox[1]+50),cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2, cv2.LINE_AA)\n",
    "            elif(emotion == \"neutral\"): #neutral\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, neutral[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg, \"neutral\",(faceBox[2]+7, faceBox[1]+50),cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2, cv2.LINE_AA)\n",
    "            elif(emotion == \"sad\" or emotion == \"disgust\" or emotion == \"fear\"): #sad\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, sad[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg, \"sad\",(faceBox[2]+7, faceBox[1]+50),cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2, cv2.LINE_AA)\n",
    "            elif(emotion == \"angry\"): #angry\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, anger[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg,\"angry\",(faceBox[2]+7, faceBox[1]+50),cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2, cv2.LINE_AA)\n",
    "            elif(emotion == \"surprise\"): #angry\n",
    "                mix_img = cv2.addWeighted(background_img[faceBox[1]-60:faceBox[1]-10, (int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25,:],0.3, anger[0:100,0:100,:],0.7,0)\n",
    "                background_img[faceBox[1]-60:faceBox[1]-10,(int((faceBox[0]+faceBox[2])/2))-25:(int((faceBox[0]+faceBox[2])/2))+25] = mix_img\n",
    "                cv2.putText(resultImg,\"angry\",(faceBox[2]+7, faceBox[1]+50),cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow('',background_img)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print('exception thrown')\n",
    "            #cv2.namedWindow('',cv2.WINDOW_NORMAL)\n",
    "            #cv2.setWindowProperty('detect age',cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "            cv2.namedWindow('',flags = cv2.WINDOW_GUI_EXPANDED)\n",
    "            cv2.imshow('',resultImg)\n",
    "            \n",
    "            \n",
    "    \n",
    "    #cv2.setWindowProperty('detect age',cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)        \n",
    "    cv2.namedWindow('',flags = cv2.WINDOW_GUI_EXPANDED)\n",
    "    cv2.imshow('',resultImg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyWindow('')\n",
    "        cv2.waitKey(1)\n",
    "        video.release\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21 in [21,22,45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
